apiVersion: nais.io/v1
kind: Alert
metadata:
  name: sykmeldingalerts
  namespace: teamsykmelding
  labels:
    team: teamsykmelding
spec:
  receivers:
    slack:
      channel: '#sykmelding-alerts-dev'
  alerts:
    - alert: syfosm-asynkron-app-nede
      expr: kube_deployment_status_replicas_available{deployment=~"syfosmaltinn|syfonlaltinn"} == 0
      for: 5m
      description: "{{ $labels.deployment }} er nede"
      action: "Se `kubectl kubectl get pod  | grep {{ $labels.deployment }}` for å se podder, og `kubectl logs $podnavn` for logger"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: syfosm-synkron-app-nede
      expr: kube_deployment_status_replicas_available{deployment=~"narmesteleder|sykmelding-gateway"} == 0
      for: 2m
      description: "{{ $labels.deployment }} er nede"
      action: "Se `kubectl kubectl get pod  | grep {{ $labels.deployment }}` for å se podder, og `kubectl logs $podnavn` for logger"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: syfosm-kontinuerlig-restart
      expr: sum(increase(kube_pod_container_status_restarts_total{container=~"syfosmaltinn|syfonlaltinn|narmesteleder|sykmelding-gateway"}[30m])) by (container) > 2
      for: 5m
      description: "{{ $labels.container }} har restartet flere ganger siste halvtimen!"
      action: "Se `kubectl describe pod {{ $labels.container }}` for events, og `kubectl logs {{ $labels.container }}` for logger"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: syfosm-mangler-metrikker
      expr: absent(up{app=~"syfosmaltinn|syfonlaltinn|narmesteleder",job="kubernetes-pods"})
      for: 5m
      description: "{{ $labels.app }} rapporterer ingen metrikker i {{ $labels.kubernetes_namespace }}"
      action: "Sjekk om {{ $labels.app }} i {{ $labels.kubernetes_namespace }} er oppe"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: syfosm-error-logging
      expr: sum(rate(logd_messages_total{log_app=~"syfosmaltinn|syfonlaltinn|narmesteleder|sykmelding-gateway",log_level="Error",job="kubernetes-pods"}[5m])) by (log_app) > 0
      for: 5m
      description: "{{ $labels.log_app }} rapporterer error i loggene"
      action: "Sjekk hvorfor {{ $labels.log_app }} har logget Error i løpet av de siste 5 minuttene"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: teamsykmeldig-app-cpu
      expr: (sum(kube_pod_container_resource_requests{namespace="teamsykmelding",resource="cpu", container!="linkerd-proxy",container!="cloudsql-proxy"}) by (container) - sum(rate(container_cpu_usage_seconds_total{namespace="teamsykmelding",container!="linkerd-proxy",container!="cloudsql-proxy",container!="POD", container!=""}[5m])) by (container))  < 0
      for: 5m
      description: "{{ $labels.container }} bruker mer CPU enn angitt"
      action: "Sjekk CPU for {{ $labels.container }}, kanskje juster opp requested eller justere opp antall pods"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: teamsykmeldig-app-ram  
      expr: sum(kube_pod_container_resource_requests{namespace="teamsykmelding", resource="memory", container!="linkerd-proxy",container!="cloudsql-proxy"}) by (container) - sum(container_memory_working_set_bytes{namespace="teamsykmelding",container!="linkerd-init",container!="linkerd-proxy",container!="cloudsql-proxy",container!="POD", container!=""}) by (container) < 0
      for: 5m
      description: "{{ $labels.container }} bruker mer RAM enn angitt"
      action: "Sjekk RAM for {{ $labels.container }}, kanskje juster opp requested eller justere opp antall pods"
      sla: respond within 1h, during office hours 
      severity: danger
