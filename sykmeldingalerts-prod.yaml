apiVersion: nais.io/v1
kind: Alert
metadata:
  name: sykmeldingalerts
  namespace: default
  labels:
    team: teamsykefravr
spec:
  receivers:
    slack:
      channel: '#sykmelding-alerts'
  alerts:
    - alert: syfosm-app-nede
      expr: kube_deployment_status_replicas_available{deployment=~"syfosmregler|syfosmregister|syfotekster|syfohelsenettproxy|syfosmpapirregler|syfosmmanuell-backend|syfosmmanuell|sykmeldinger-backend|pale-2-regler|smregistrering|smregistrering-backend|syfonarmesteleder"} == 0
      for: 2m
      description: "{{ $labels.deployment }} er nede"
      action: "Se `kubectl kubectl get pod  | grep {{ $labels.deployment }}` for å se podder, og `kubectl logs $podnavn` for logger"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: syfosm-asynkron-app-nede
      expr: kube_deployment_status_replicas_available{deployment=~"syfosmoppgave|syfoservice-mq-producer|syfosmarena|syfosmvarsel|syfosminfotrygd|syfosmsak|syfosmmottak|syfosmapprec|syfosmpapirmottak|pale-2|pale-2-sak|pale-2-register|sparenaproxy"} == 0
      for: 10m
      description: "{{ $labels.deployment }} er nede"
      action: "Se `kubectl kubectl get pod  | grep {{ $labels.deployment }}` for å se podder, og `kubectl logs $podnavn` for logger"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: syfosm-kontinuerlig-restart
      expr: sum(increase(kube_pod_container_status_restarts_total{container=~"syfosmregler|syfoservice-mq-producer|syfosminfotrygd|syfosmregister|syfosmoppgave|syfosmarena|syfosmvarsel|syfosmsak|syfosmmottak|syfosmapprec|syfosmpapirmottak|syfosmpapirregler|syfonarmesteleder|syfosmmanuell-backend|syfosmmanuell|sykmeldinger-backend|pale-2|pale-2-sak|pale-2-register|pale-2-regler|sparenaproxy|smregistrering|smregistrering-backend"}[30m])) by (container) > 2
      for: 5m
      description: "{{ $labels.container }} har restartet flere ganger siste halvtimen!"
      action: "Se `kubectl describe pod {{ $labels.container }}` for events, og `kubectl logs {{ $labels.container }}` for logger"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: syfosm-mangler-metrikker
      expr: absent(up{app=~"syfosmoppgave|syfoservice-mq-producer|syfosmarena|syfosmvarsel|syfosminfotrygd|syfosmregister|syfosmsak|syfosmmottak|syfosmregler|syfohelsenettproxy|syfosmapprec|syfosmpapirmottak|syfosmpapirregler|syfonarmesteleder|syfosmmanuell-backend|sykmeldinger-backend|pale-2|pale-2-sak|pale-2-regler|sparenaproxy|smregistrering|smregistrering-backend",job="kubernetes-pods"})
      for: 5m
      description: "{{ $labels.app }} rapporterer ingen metrikker i {{ $labels.kubernetes_namespace }}"
      action: "Sjekk om {{ $labels.app }} i {{ $labels.kubernetes_namespace }} er oppe"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: syfosmmottak-ugyldige-sykmeldinger
      expr: sum(increase(syfosmmottak_invalid_message_no_notice_count[10m])) > 50
      for: 10m
      description: "Syfosmmottak har mottatt unormalt mange ugyldige sykmeldinger"
      action: "Sjekk logger, og om det kan ha blitt innført feil"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: syfosm-error-logging
      expr: sum(rate(logd_messages_total{log_app=~"syfosmoppgave|syfoservice-mq-producer|syfosmarena|syfosmvarsel|syfosminfotrygd|syfosmregister|syfosmsak|syfosmmottak|syfosmregler|syfohelsenettproxy|syfosmapprec|syfosmpapirmottak|syfosmpapirregler|syfonarmesteleder|syfosmmanuell-backend|syfosmmanuell|sykmeldinger-backend|pale-2|pale-2-sak|pale-2-register|pale-2-regler|sparenaproxy|smregistrering|smregistrering-backend",log_level="Error",job="kubernetes-pods"}[5m])) by (log_app) > 0
      for: 5m
      description: "{{ $labels.log_app }} rapporterer error i loggene"
      action: "Sjekk hvorfor {{ $labels.log_app }} har i løpet av dei siste 5 minuttene logget Error"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: syfosminfotrygd-smikkeokko
      expr: syfosminfotrygd_smikkeok_gauge{log_app=~"syfosminfotrygd"} > 0
      for: 3m
      description: "Meldinger er kommet inn på QA.P414.AS07_SMIKKEOK"
      action: "Sjekk mq køen: QA.P414.AS07_SMIKKEOK, og se på meldinge som har lagt seg der, og kontakt infotrygd teamet, få å finne ut hvorfor den har blitt lagt der"
    - alert: syfosmregler-avisst-sykmeldinger
      expr: sum(increase(syfosmregler_rule_hit_status_counter{app=~"syfosmregler",rule_status="INVALID"}[30m])) by (rule_status) > 120
      for: 10m
      description: "Syfosmregler har mottatt unormalt mange avisste sykmeldinger"
      action: "Sjekk logger, og om det kan ha blitt innført feil"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: syfosminfotrygd-mange-manuelle-oppg
      expr: (100 * sum(rate(syfosminfotrygd_rule_hit_status_counter{rule_status="MANUAL_PROCESSING"}[10m])) / sum(rate(syfosminfotrygd_rule_hit_status_counter[10m]))) > 90
      for: 10m
      description: "Syfosminfotrygd oppretter unormalt mange oppgaver"
      action: "Sjekk logger, kanskje Infotrygd har problemer"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: pale-2-ugyldige-legeerklaeringer
      expr: sum(increase(pale2_invalid_message_no_notice_count[10m])) > 20
      for: 10m
      description: "Pale-2 har mottatt unormalt mange ugyldige legeerklæringer"
      action: "Sjekk logger, og om det kan ha blitt innført feil"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: syfosmsak-kafka-offset
      expr: kafka_consumergroup_group_sum_lag{group=~"syfosmsak|syfosmsak-consumer"} > 500
      for: 1m
      description: "Syfosmsak kafka-lag er høyt"
      action: "Sjekk om Syfosmsak leser fra kafka"
      sla: respond within 1h, during office hours
      severity: danger
